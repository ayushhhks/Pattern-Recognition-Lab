{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498efa25-3fa6-4208-b738-e1bcbc3c5f6b",
   "metadata": {},
   "source": [
    "Alright, let‚Äôs break this code down in **simple ‚Äúexplain like I‚Äôm 10‚Äù terms** üß©:\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What the code does\n",
    "It calculates **Shannon entropy** for a column in the Titanic dataset.  \n",
    "Entropy here is a fancy word for **how unpredictable or ‚Äúmixed up‚Äù the values are**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Step-by-step explanation\n",
    "\n",
    "1. **Imports tools**  \n",
    "   - `math` ‚Üí for math functions like logarithms.  \n",
    "   - `Counter` ‚Üí counts how many times each value appears.  \n",
    "   - `pandas` ‚Üí helps load and work with the Titanic dataset.\n",
    "\n",
    "2. **Shannon entropy function**  \n",
    "   ```python\n",
    "   def shannon_entropy(values):\n",
    "       counter = Counter(values)   # count each unique value\n",
    "       total = sum(counter.values())  # total number of values\n",
    "       entropy = 0.0\n",
    "       stats = []\n",
    "       \n",
    "       for val, count in counter.items():\n",
    "           prob = count / total   # probability of each value\n",
    "           entropy -= prob * math.log2(prob)  # entropy formula\n",
    "           stats.append((val, count, prob))   # keep stats\n",
    "       \n",
    "       return entropy, stats\n",
    "   ```\n",
    "   - Think of it like:  \n",
    "     If you roll a dice, each side has a probability. Entropy measures how ‚Äúsurprising‚Äù the roll is.  \n",
    "     - If all rolls are the same ‚Üí entropy = 0 (no surprise).  \n",
    "     - If rolls are evenly spread ‚Üí entropy is high (lots of surprise).\n",
    "\n",
    "3. **Load Titanic dataset**  \n",
    "   ```python\n",
    "   url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "   df = pd.read_csv(url)\n",
    "   ```\n",
    "   - Pulls Titanic passenger data from GitHub.  \n",
    "   - Example columns: `Name`, `Sex`, `Age`, `Survived`.\n",
    "\n",
    "4. **Show first 5 rows**  \n",
    "   ```python\n",
    "   print(df.head())\n",
    "   ```\n",
    "   - Just a preview of the data.\n",
    "\n",
    "5. **Ask user for a column**  \n",
    "   ```python\n",
    "   col = input(\"Enter column name...\")\n",
    "   ```\n",
    "   - You type a column name (like `\"Sex\"` or `\"Survived\"`).\n",
    "\n",
    "6. **Check if column is valid**  \n",
    "   - If not in dataset ‚Üí prints ‚ÄúInvalid column name.‚Äù  \n",
    "   - If valid ‚Üí calculates entropy.\n",
    "\n",
    "7. **Calculate entropy**  \n",
    "   ```python\n",
    "   values = df[col].dropna().astype(str).tolist()\n",
    "   entropy, stats = shannon_entropy(values)\n",
    "   ```\n",
    "   - Drops missing values.  \n",
    "   - Converts everything to strings.  \n",
    "   - Runs the entropy function.\n",
    "\n",
    "8. **Print results**  \n",
    "   - Shows each unique value, how many times it appears, and its probability.  \n",
    "   - Finally prints the entropy score.\n",
    "\n",
    "---\n",
    "\n",
    "### üé≤ Example\n",
    "If you choose column `\"Sex\"`:\n",
    "- Values: `male`, `female`  \n",
    "- Counts: say 577 males, 314 females  \n",
    "- Probabilities: ~0.65 male, ~0.35 female  \n",
    "- Entropy: around **0.93** (not max surprise, but some mix).\n",
    "\n",
    "---\n",
    "\n",
    "### üß© In kid-friendly terms\n",
    "Imagine you have a bag of candies:\n",
    "- If all candies are the same flavor ‚Üí boring, entropy = 0.  \n",
    "- If candies are mixed (chocolate, strawberry, mint) ‚Üí more exciting, entropy goes up.  \n",
    "This code measures that excitement level for Titanic data columns.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to **run this code on the Titanic dataset right now** and show you the entropy for a few interesting columns (like `Survived`, `Sex`, `Pclass`)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c5cf7-0910-4060-9c34-7adcf7eec6cf",
   "metadata": {},
   "source": [
    "Great question! Let‚Äôs unpack this clearly:\n",
    "\n",
    "---\n",
    "\n",
    "### üåç What is **Entropy**?\n",
    "- In everyday language, **entropy** means *disorder* or *uncertainty*.  \n",
    "- Imagine a messy room: high entropy. A perfectly tidy room: low entropy.  \n",
    "- In information theory, entropy measures **how unpredictable data is**.  \n",
    "  - If you flip a coin that always lands on heads ‚Üí entropy = 0 (no surprise).  \n",
    "  - If the coin is fair (50/50 heads or tails) ‚Üí entropy is high (maximum surprise).\n",
    "\n",
    "---\n",
    "\n",
    "### üìä What is **Shannon Entropy**?\n",
    "- Proposed by **Claude Shannon**, the ‚Äúfather of information theory.‚Äù  \n",
    "- It‚Äôs a formula that tells us **how much information (or surprise) is in a dataset**.  \n",
    "- Formula:\n",
    "\n",
    "\\[\n",
    "H = - \\sum_{i=1}^{n} p_i \\cdot \\log_2(p_i)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(p_i\\) = probability of each unique value.  \n",
    "- The sum goes over all possible values.  \n",
    "- The negative sign makes the result positive.\n",
    "\n",
    "---\n",
    "\n",
    "### üé≤ Example\n",
    "Suppose you have a bag of candies:\n",
    "- 100% chocolate ‚Üí entropy = 0 (no surprise, always chocolate).  \n",
    "- 50% chocolate, 50% strawberry ‚Üí entropy = 1 bit (maximum surprise for 2 outcomes).  \n",
    "- 33% chocolate, 33% strawberry, 34% mint ‚Üí entropy ‚âà 1.58 bits (more variety, more surprise).\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Why it matters\n",
    "- **Data science**: Helps measure diversity in data (like Titanic passengers‚Äô genders or survival outcomes).  \n",
    "- **Compression**: High entropy data is harder to compress.  \n",
    "- **Cryptography**: Randomness (high entropy) makes systems more secure.  \n",
    "- **Machine learning**: Used in decision trees (information gain).\n",
    "\n",
    "---\n",
    "\n",
    "üëâ So, in short:  \n",
    "- **Entropy** = measure of uncertainty or surprise.  \n",
    "- **Shannon entropy** = the mathematical way to calculate that surprise in information.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show you **Shannon entropy values for a few Titanic dataset columns** (like `Sex`, `Survived`, `Pclass`) so you can see how it works in practice?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b745616f-b4d8-424a-9e5f-c71bc6baac9b",
   "metadata": {},
   "source": [
    "### WAP to compute shanon entropy for a column from a given dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe988bb-8c46-45ce-89e1-80e2eb4736b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of Titanic dataset:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "Available columns:\n",
      " ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter column name to calculate Shannon entropy:  Survived\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value statistics:\n",
      "Value\tCount\tProbability\n",
      "0\t549\t0.6162\n",
      "1\t342\t0.3838\n",
      "\n",
      "Shannon Entropy for column 'Survived' = 0.9607\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# --- Shannon entropy function ---\n",
    "def shannon_entropy(values):\n",
    "    counter = Counter(values)\n",
    "    total = sum(counter.values())\n",
    "    entropy = 0.0\n",
    "    stats = []\n",
    "    \n",
    "    for val, count in counter.items():\n",
    "        prob = count / total\n",
    "        entropy -= prob * math.log2(prob)\n",
    "        stats.append((val, count, prob))\n",
    "    \n",
    "    return entropy, stats\n",
    "\n",
    "# --- Load Titanic dataset from GitHub ---\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Show first 5 rows\n",
    "print(\"First 5 rows of Titanic dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# --- Ask user for column name ---\n",
    "print(\"\\nAvailable columns:\\n\", list(df.columns))\n",
    "col = input(\"\\nEnter column name to calculate Shannon entropy: \")\n",
    "\n",
    "if col not in df.columns:\n",
    "    print(\"Invalid column name.\")\n",
    "else:\n",
    "    values = df[col].dropna().astype(str).tolist()\n",
    "    entropy, stats = shannon_entropy(values)\n",
    "    \n",
    "    print(\"\\nValue statistics:\")\n",
    "    print(\"Value\\tCount\\tProbability\")\n",
    "    for val, count, prob in stats:\n",
    "        print(f\"{val}\\t{count}\\t{prob:.4f}\")\n",
    "    \n",
    "    print(f\"\\nShannon Entropy for column '{col}' = {entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bede1802-7abb-4610-83c8-7d79a0820068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
